# Provisioning Kubernetes Cluster with GPU Support

This sample project helps you provision a Kubernetes Cluster with the GPU support. It uses [ACS-Engine](https://github.com/Azure/acs-engine) to generate the Azure Resource Management templates, and then deploys the resources to your subscription and resource group.

## Prerequisites
- Install [Docker](https://docs.docker.com/engine/installation/)
    - If on Windows, make sure enable linux containers
    - Also, please make sure to share the the drive with Docker in the settings

## Provision

First, build and start the Docker container, which takes care of setting up the prerequisites for you (such as downloading ACS-Engine, Azure-CLI, helm, and kubectl).

Navigate to the provisioning folder. Then run:

``` bash
# Run for Windows
./StartDocker.ps1

# Run for Linux/Mac
./StartDocker.sh
```

This will start up a docker instance with all the tools needed for provisioning and managing the cluster, and it will start a bash shell.

Then, make edits to the [kubernetes.json](/deploy/kubernetes.json) file as necessary (the sample can be run with no changes). This is where you can change the number of GPU and CPU nodes that you would like to have in your cluster. Feel free to leave the DNS prefix, SSH key and service principal blank, and those will be autogenerated for you.

If autogenerating the service principal, please make sure that you have owner or role assignment permissions on the resource group or subscription. Otherwise, the script will not be able to give proper permissions to the service principal.

To run provisioning:

``` bash
./deploy/deploy.sh <subscription_id> <resource_group> <location>
```

The script will create resources for a kubernetes cluster with Ubuntu 16.04 LTS as the base image. To see the assets that it generated, go to provisioning/deploy/_output/autocluster{latesttime}/. 

In addition to the cluster, this sample also creates a storage account and file share for you, so that it is ready to be used with the distributed tensorflow sample. The storage account created will be called autoclusterpod{numbers}. We add a datetime based number to the name, so that you should rarely have name conflicts.

We also run GPU driver installation as a custom script step. Currently, we only support NC vms for our sample. Once ACS-Engine releases their version 0.9.0, it should support driver installation automatically for both NC and NV vms. To switch to the new version once it comes out, update the [DockerFile](DockerFile) to have 0.9.0 as the ACS-Engine version, and rebuild the docker image. 

The custom script runner can be used if you need to run jobs across all GPU machines (such as for maintenance or to update the NVIDIA driver version, etc.). Check out the [provision-custom-script.sh](scripts/provision-custom-script.sh) script to see how to use as separate utility. To run your own script, replace the GitHub gist in [vm-custom-script.json](scripts/provision-custom-script.sh) with your own raw Gist URL.

## Teardown

Once you are done with the cluster, you can choose to either delete everything (including the resource group), or just delete the contents of the resource group, leaving the resource group itself and its access rules intact.   

```bash
./teardown/delete-all.sh <subscription_id> <resource_group>

./teardown/delete-contents.sh <subscription_id> <resource_group>
```
